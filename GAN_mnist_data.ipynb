{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMHYcgO5xT4LrxkeHCZws7N",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MParsaMo/Generative-Adversarial-Network-GAN-on-MNIST-Digits/blob/main/GAN_mnist_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Q5w0AQOHBkm7"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Input, Dense, Dropout, Activation, Flatten\n",
        "from keras.layers import LeakyReLU\n",
        "from keras.optimizers import Adam\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Enable eager execution\n",
        "tf.config.run_functions_eagerly(True)\n",
        "\n",
        "# only the X_train (the actual images) is actively used from the MNIST dataset but we have to load all the data\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "#printing x_shapes befor reshaping\n",
        "print(f'x_train.shape befor reshape : {x_train.shape}')\n",
        "print(f'x_test.shape befor reshape : {x_test.shape}')\n",
        "\n",
        "# we reshape the images(flatten it from 28*28 into 784)\n",
        "x_train = x_train.reshape(-1, 784)\n",
        "x_test = x_test.reshape(-1, 784)\n",
        "\n",
        "#printing x_shapes after reshaping\n",
        "print(f'x_train.shape after reshape : {x_train.shape}')\n",
        "print(f'x_test.shape after reshape : {x_test.shape}')\n",
        "\n",
        "# normalization\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "#__________________ now or features are ready________________________________\n",
        "\n",
        "# set the dimensions of the noise (latent) vector\n",
        "z_dim =100\n",
        "\n",
        "# define a dictionary to store the loss-values\n",
        "losses = {\"D\": [], \"G\": []}\n",
        "\n",
        "# Optimizer for the generator\n",
        "# when we set discriminator.trainable = False and then try to train the gan model,\n",
        "# the adam optimizer instance gets confused because its internal state is tied to the variables it was originally initialized to optimize.\n",
        "# It then encounters variables (from the now-frozen discriminator) that it's no longer supposed to update but were part of its initial configuration, leading to the \"Unknown variable\" error.\n",
        "adam_generator = Adam(learning_rate=0.0002, beta_1=0.5)\n",
        "# Optimizer for the discriminator\n",
        "adam_discriminator = Adam(learning_rate=0.0002, beta_1=0.5)\n",
        "\n",
        "\n",
        "# the generator transform the low-dimensional z random vector to a high dimensional vector (fake image) from 100 to 784\n",
        "generator = Sequential()\n",
        "generator.add(Dense(256, input_dim=z_dim, activation=LeakyReLU(alpha=0.2)))\n",
        "generator.add(Dense(512, activation=LeakyReLU(alpha=0.2)))\n",
        "generator.add(Dense(784, activation='sigmoid'))\n",
        "generator.compile(loss='binary_crossentropy', optimizer=adam_generator, metrics=['accuracy']) # Use adam_generator\n",
        "generator.summary()\n",
        "\n",
        "# the discriminator output is a value in the range [0,1]\n",
        "# 0: image is fake\n",
        "# 1: image is real\n",
        "discriminator = Sequential()\n",
        "discriminator.add(Dense(512, input_dim=784, activation=LeakyReLU(alpha=0.2)))\n",
        "discriminator.add(Dropout(0.3))\n",
        "discriminator.add(Dense(256, activation=LeakyReLU(alpha=0.2)))\n",
        "discriminator.add(Dropout(0.3))\n",
        "discriminator.add(Dense(1, activation='sigmoid'))\n",
        "discriminator.compile(loss='binary_crossentropy', optimizer=adam_discriminator, metrics=['accuracy']) # Use adam_discriminator\n",
        "discriminator.summary()\n",
        "\n",
        "# for updating weights in generator we need to creat a model with generator and discriminator. but the discriminator should not be updated.\n",
        "# Compile the GAN with the discriminator trainable flag set to False\n",
        "# Note: For GAN, the combined model's optimizer should *only* optimize the generator's weights.\n",
        "# The discriminator's weights should be frozen when training the combined GAN model.\n",
        "discriminator.trainable = False # Freeze discriminator for the combined GAN model training\n",
        "gan_input = Input(shape=(z_dim,))\n",
        "gan_hidden = generator(gan_input)\n",
        "gan_output = discriminator(gan_hidden)\n",
        "gan = Model(inputs=gan_input, outputs=gan_output)\n",
        "gan.compile(loss='binary_crossentropy', optimizer=adam_generator, metrics=['accuracy']) # Use adam_generator here as it only optimizes generator part\n",
        "gan.summary()\n",
        "\n",
        "# loss_values contains the (loss, accuracy) for every epoch\n",
        "def plot_loss(loss_values):\n",
        "    # index 0: loss (this is what we need)\n",
        "    # index 1: accuracy\n",
        "    print(loss_values)\n",
        "    d_loss = [v[0] for v in loss_values[\"D\"]]\n",
        "    g_loss = [v[0] for v in loss_values[\"G\"]]\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    plt.plot(d_loss, label=\"Discriminator loss\")\n",
        "    plt.plot(g_loss, label=\"Generator loss\")\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_fake_images(n=10, dim=(1, 10)):\n",
        "    noise = np.random.normal(0, 1, size=(n, z_dim))\n",
        "    generated_images = generator.predict(noise)\n",
        "    generated_images = generated_images.reshape(n, 28, 28)\n",
        "\n",
        "    plt.figure(figsize=(12, 2))\n",
        "    for i in range(generated_images.shape[0]):\n",
        "        plt.subplot(dim[0], dim[1], i + 1)\n",
        "        plt.imshow(generated_images[i], interpolation='nearest', cmap='gray_r')\n",
        "        plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def train(epochs=1, batch_size=128):\n",
        "    batch_count = x_train.shape[0] // batch_size\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        print(f\"Epoch {epoch}\")\n",
        "        for batch_i in range(batch_count):\n",
        "            # ---------------------\n",
        "            #  Train Discriminator\n",
        "            # ---------------------\n",
        "\n",
        "            # get a random batch of real images\n",
        "            image_batch = x_train[np.random.randint(0, x_train.shape[0], size=batch_size)]\n",
        "\n",
        "            # generate a batch of fake images\n",
        "            noise = np.random.normal(0, 1, size=(batch_size, z_dim))\n",
        "            generated_images = generator.predict(noise)\n",
        "\n",
        "            # set features and targets for discriminator\n",
        "            discriminator_features = np.concatenate((image_batch, generated_images))\n",
        "            # Real images are label 0.9 (soft labels), fake images are label 0\n",
        "            discriminator_targets = np.zeros((2 * batch_size, 1))\n",
        "            discriminator_targets[:batch_size] = 0.9\n",
        "\n",
        "            # Train discriminator on batch\n",
        "            # Ensure discriminator is trainable when training it directly\n",
        "            discriminator.trainable = True\n",
        "            d_loss = discriminator.train_on_batch(discriminator_features, discriminator_targets)\n",
        "            losses[\"D\"].append(d_loss)\n",
        "\n",
        "            # ---------------------\n",
        "            #  Train Generator\n",
        "            # ---------------------\n",
        "\n",
        "            # Generate new noise for the generator's training step\n",
        "            # This forces the generator to produce a completely new set of fake images,\n",
        "            # ensuring it learns a general mapping from latent space to image space.\n",
        "            new_noise = np.random.normal(0, 1, size=(batch_size, z_dim))\n",
        "            # Generator wants discriminator to classify fake images as real (label 1)\n",
        "            generator_targets = np.ones((batch_size, 1)) # Labels for generator training are 1 (real)\n",
        "\n",
        "            # Train generator on batch (discriminator is frozen here as per `gan` model setup)\n",
        "            # Ensure discriminator is NOT trainable when training the combined GAN model\n",
        "            # This is handled by `discriminator.trainable = False` before `gan` compilation\n",
        "            g_loss = gan.train_on_batch(new_noise, generator_targets)\n",
        "            losses[\"G\"].append(g_loss)\n",
        "\n",
        "    # we create 10 fake images after the training\n",
        "    plot_fake_images()\n",
        "    plot_loss(losses)\n",
        "\n",
        "train(epochs=10, batch_size=128)"
      ]
    }
  ]
}